{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/refusal_direction/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "def dump_json(data, file_path):\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.abspath(\"\")\n",
    "raw_data_dir = os.path.join(current_dir, 'raw')\n",
    "processed_data_dir = os.path.join(current_dir, 'processed')\n",
    "\n",
    "def download_advbench():\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    raw_file_path = os.path.join(raw_data_dir, 'advbench.csv')\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'advbench.json')\n",
    "\n",
    "    download_file(url, raw_file_path)\n",
    "    dataset = pd.read_csv(raw_file_path)\n",
    "\n",
    "    instructions = dataset['goal'].to_list()\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': None} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_malicious_instruct():\n",
    "    url = 'https://raw.githubusercontent.com/Princeton-SysML/Jailbreak_LLM/main/data/MaliciousInstruct.txt'\n",
    "    raw_file_path = os.path.join(raw_data_dir, 'malicious_instruct.txt')\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'malicious_instruct.json')\n",
    "\n",
    "    download_file(url, raw_file_path)\n",
    "    with open(raw_file_path, 'r') as f:\n",
    "        instructions = f.readlines()\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': None} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_tdc2023():\n",
    "    urls = [\n",
    "        'https://raw.githubusercontent.com/centerforaisafety/tdc2023-starter-kit/main/red_teaming/data/dev/behaviors.json',\n",
    "        'https://raw.githubusercontent.com/centerforaisafety/tdc2023-starter-kit/main/red_teaming/data/test/behaviors.json'\n",
    "    ]\n",
    "    raw_file_paths = [\n",
    "        os.path.join(raw_data_dir, 'tdc2023_dev_behaviors.json'),\n",
    "        os.path.join(raw_data_dir, 'tdc2023_test_behaviors.json')\n",
    "    ]\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'tdc2023.json')\n",
    "\n",
    "    for url, raw_file_path in zip(urls, raw_file_paths):\n",
    "        download_file(url, raw_file_path)\n",
    "\n",
    "    instructions = []\n",
    "    for raw_file_path in raw_file_paths:\n",
    "        with open(raw_file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        instructions.extend(dataset)\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': None} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_jailbreakbench():\n",
    "    url = 'https://raw.githubusercontent.com/JailbreakBench/jailbreakbench/main/src/jailbreakbench/data/behaviors.csv'\n",
    "    raw_file_path = os.path.join(raw_data_dir, 'jailbreakbench.csv')\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'jailbreakbench.json')\n",
    "\n",
    "    download_file(url, raw_file_path)\n",
    "    dataset = pd.read_csv(raw_file_path)\n",
    "\n",
    "    instructions = dataset['Goal'].to_list()\n",
    "    categories = dataset['Category'].to_list()\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': category} for instruction, category in zip(instructions, categories)]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_harmbench(split):\n",
    "    assert split in ['val', 'test']\n",
    "\n",
    "    url = f'https://raw.githubusercontent.com/centerforaisafety/HarmBench/main/data/behavior_datasets/harmbench_behaviors_text_{split}.csv'\n",
    "    raw_file_path = os.path.join(raw_data_dir, f'harmbench_{split}.csv')\n",
    "    processed_file_path = os.path.join(processed_data_dir, f'harmbench_{split}.json')\n",
    "\n",
    "    download_file(url, raw_file_path)\n",
    "    dataset = pd.read_csv(raw_file_path)\n",
    "\n",
    "    instructions = []\n",
    "    categories = []\n",
    "    # filter out instructions with Category=copyright or Tags=context\n",
    "    filtered_dataset = dataset[\n",
    "        ~dataset['FunctionalCategory'].str.contains('copyright', case=False, na=False) &\n",
    "        ~dataset['Tags'].str.contains('context', case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    instructions.extend(filtered_dataset['Behavior'].to_list())\n",
    "    categories.extend(filtered_dataset['SemanticCategory'].to_list())\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': category} for instruction, category in zip(instructions, categories)]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_strongreject():\n",
    "    url = 'https://raw.githubusercontent.com/alexandrasouly/strongreject/main/strongreject_dataset/strongreject_dataset.csv'\n",
    "    raw_file_path = os.path.join(raw_data_dir, 'strongreject.csv')\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'strongreject.json')\n",
    "\n",
    "    download_file(url, raw_file_path)\n",
    "    dataset = pd.read_csv(raw_file_path)\n",
    "\n",
    "    instructions = dataset['forbidden_prompt'].to_list()\n",
    "    categories = dataset['category'].to_list()\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': category} for instruction, category in zip(instructions, categories)]\n",
    "    dump_json(dataset_json, processed_file_path)\n",
    "\n",
    "def download_alpaca():\n",
    "    hf_path = 'tatsu-lab/alpaca'\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'alpaca.json')\n",
    "\n",
    "    dataset = load_dataset(hf_path)\n",
    "\n",
    "    # filter for instructions that do not have inputs\n",
    "    instructions = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if dataset['train'][i]['input'].strip() == '':\n",
    "            instructions.append(dataset['train'][i]['instruction'])\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': None} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_advbench()\n",
    "download_malicious_instruct()\n",
    "download_tdc2023()\n",
    "\n",
    "download_jailbreakbench()\n",
    "download_harmbench(split='val')\n",
    "download_harmbench(split='test')\n",
    "download_strongreject()\n",
    "\n",
    "download_alpaca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct train, val, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.abspath(\"\")\n",
    "splits_data_dir = os.path.join(current_dir, 'splits')\n",
    "\n",
    "max_train_subset_size = 128 # limits the number of examples from a single dataset\n",
    "\n",
    "def construct_harmful_dataset_splits():\n",
    "    harmful_train_path = os.path.join(splits_data_dir, 'harmful_train.json')\n",
    "    harmful_val_path = os.path.join(splits_data_dir, 'harmful_val.json')\n",
    "    harmful_test_path = os.path.join(splits_data_dir, 'harmful_test.json')\n",
    "\n",
    "    harmful_train_instructions = []\n",
    "    for file in ['advbench.json', 'malicious_instruct.json', 'tdc2023.json']:\n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if len(data) > max_train_subset_size:\n",
    "                data = random.sample(data, max_train_subset_size)\n",
    "            harmful_train_instructions.extend(data)\n",
    "        \n",
    "    harmful_val_instructions = []\n",
    "    for file in ['harmbench_val.json']:\n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            harmful_val_instructions.extend(json.load(f))\n",
    "    \n",
    "    harmful_test_instructions = []\n",
    "    for file in ['jailbreakbench.json', 'harmbench_test.json', 'strongreject.json']:\n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            harmful_test_instructions.extend(json.load(f))\n",
    "\n",
    "    # now ensure that there are no duplicates across datasets\n",
    "    filtered_out = 0\n",
    "    filtered_harmful_train_instructions = []\n",
    "    for instruction in harmful_train_instructions:\n",
    "        if instruction['instruction'] not in [x['instruction'] for x in harmful_val_instructions] and instruction['instruction'] not in [x['instruction'] for x in harmful_test_instructions]:\n",
    "            filtered_harmful_train_instructions.append(instruction)\n",
    "        else:\n",
    "            filtered_out += 1\n",
    "\n",
    "    print(f\"filtered out {filtered_out} duplicate instructions from train set\")\n",
    "    harmful_train_instructions = filtered_harmful_train_instructions\n",
    "\n",
    "    filtered_out = 0\n",
    "    filtered_harmful_val_instructions = []\n",
    "    for instruction in harmful_val_instructions:\n",
    "        if instruction['instruction'] not in [x['instruction'] for x in harmful_test_instructions]:\n",
    "            filtered_harmful_val_instructions.append(instruction)\n",
    "        else:\n",
    "            filtered_out += 1\n",
    "    \n",
    "    print(f\"filtered out {filtered_out} duplicate instructions from val set\")\n",
    "    harmful_val_instructions = filtered_harmful_val_instructions\n",
    "\n",
    "    dump_json(harmful_train_instructions, harmful_train_path)\n",
    "    dump_json(harmful_val_instructions, harmful_val_path)\n",
    "    dump_json(harmful_test_instructions, harmful_test_path)\n",
    "\n",
    "def construct_harmless_dataset_splits():\n",
    "    harmless_train_path = os.path.join(splits_data_dir, 'harmless_train.json')\n",
    "    harmless_val_path = os.path.join(splits_data_dir, 'harmless_val.json')\n",
    "    harmless_test_path = os.path.join(splits_data_dir, 'harmless_test.json')\n",
    "\n",
    "    train_p, val_p, test_p = 0.6, 0.20, 0.20\n",
    "\n",
    "    harmless_instructions = []\n",
    "    for file in ['alpaca.json']:\n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            harmless_instructions.extend(json.load(f))\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(harmless_instructions)\n",
    "\n",
    "    total_size = len(harmless_instructions)\n",
    "    train_size = int(train_p * total_size)\n",
    "    val_size = int(val_p * total_size)\n",
    "\n",
    "    harmless_train_instructions = harmless_instructions[:train_size]\n",
    "    harmless_val_instructions = harmless_instructions[train_size:train_size+val_size]\n",
    "    harmless_test_instructions = harmless_instructions[train_size+val_size:]\n",
    "\n",
    "    dump_json(harmless_train_instructions, harmless_train_path)\n",
    "    dump_json(harmless_val_instructions, harmless_val_path)\n",
    "    dump_json(harmless_test_instructions, harmless_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered out 68 duplicate instructions from train set\n",
      "filtered out 2 duplicate instructions from val set\n"
     ]
    }
   ],
   "source": [
    "construct_harmful_dataset_splits()\n",
    "construct_harmless_dataset_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
