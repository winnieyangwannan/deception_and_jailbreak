The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/winnieyangwn/deception_and_jailbreak/src/submodules/truthfulQA
current_directory: /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/winnieyangwn/deception_and_jailbreak/src/submodules/truthfulQA
Using 1 GPUs
Process rank: 0
cfg: Config(model_path='Meta-Llama-3-8B-Instruct', model_name='Meta-Llama-3-8B-Instruct', save_dir='/home/winnieyangwn/Output', n_positions=1, n_train=0, n_test=0, batch_size=32, max_new_tokens=100, task_name='truthful_qa', steer_type='negative_addition', source_layer=14, target_layer=14, contrastive_type=('factual', 'misconception'), steering_strength=1, label_type=('A', 'B'))
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 790 examples [00:00, 15354.78 examples/s]
Train size: 632
Test size: 158
Train size (SUBSAMPLE): 632
Test size (SUBSAMPLE): 158
Traceback (most recent call last):
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/Meta-Llama-3-8B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/transformers/utils/hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 458, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-67e84020-394ff68c737dc5d352b89706;0b814d67-b573-4a3a-b0c7-d4c3dc13635c)

Repository Not Found for url: https://huggingface.co/Meta-Llama-3-8B-Instruct/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/winnieyangwn/deception_and_jailbreak/src/submodules/truthfulQA/run_truthfulqa_generation.py", line 137, in <module>
    main(model_path=args.model_path,
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/winnieyangwn/deception_and_jailbreak/src/submodules/truthfulQA/run_truthfulqa_generation.py", line 123, in main
    model_base = TransformerModelLoader(model_path, accelerator=accelerator)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/winnieyangwn/deception_and_jailbreak/src/submodules/truthfulQA/MODEL/model_base.py", line 41, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/transformers/utils/hub.py", line 365, in cached_file
    raise EnvironmentError(
OSError: Meta-Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Traceback (most recent call last):
  File "/home/winnieyangwn/miniconda3/envs/refat/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1194, in launch_command
    simple_launcher(args)
  File "/home/winnieyangwn/miniconda3/envs/refat/lib/python3.11/site-packages/accelerate/commands/launch.py", line 780, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/winnieyangwn/miniconda3/envs/refat/bin/python3.11', 'run_truthfulqa_generation.py', '--model_path=Meta-Llama-3-8B-Instruct']' returned non-zero exit status 1.
srun: error: cr6-p548xlarge-9: task 0: Exited with exit code 1
srun: Terminating StepId=129056.0
