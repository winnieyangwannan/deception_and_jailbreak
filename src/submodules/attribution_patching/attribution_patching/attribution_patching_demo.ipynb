{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T09:13:27.966033Z",
     "start_time": "2024-09-08T09:13:27.955062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pprint\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set, Union\n",
    "import numpy as np\n",
    "from matplotlib.widgets import EllipseSelector\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import einops\n",
    "import os\n",
    "import argparse\n",
    "# from pipeline.configs.config_generation import Config\n",
    "from datasets import load_dataset\n",
    "import sae_lens\n",
    "import pysvelte\n",
    "from torchtyping import TensorType as TT\n",
    "from IPython import get_ipython\n",
    "from pathlib import Path\n",
    "\n",
    "# ipython = get_ipython()\n",
    "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "# ipython.magic(\"load_ext autoreload\")\n",
    "# ipython.magic(\"autoreload 2\")\n",
    "from IPython.display import HTML, Markdown\n",
    "from sae_lens import HookedSAETransformer\n",
    "from tqdm import tqdm\n",
    "# from src.submodules.evaluations.run_evaluate_generation_deception import (\n",
    "#     plot_lying_honest_performance,\n",
    "# )\n",
    "# from src.submodules.evaluations.run_evaluate_generation_deception import (\n",
    "#     evaluate_generation_deception,\n",
    "# )\n",
    "from functools import partial\n",
    "import torch\n",
    "# from src.submodules.dataset.task_and_dataset_deception import (\n",
    "#     load_and_sample_datasets,\n",
    "#     construct_prompt,\n",
    "# )\n",
    "# from src.submodules.model.load_model import load_model\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "# from src.utils.plotly_utils import imshow, line, scatter, bar\n",
    "import plotly.io as pio\n",
    "import circuitsvis as cv  # for visualize attetnion pattern\n",
    "\n",
    "from IPython.display import display, HTML  # for visualize attetnion pattern\n"
   ],
   "id": "94b360a3c2fcbedd",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jaxtyping'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-60487a3560b1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwidgets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEllipseSelector\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mjaxtyping\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mFloat\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mInt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0meinops\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'jaxtyping'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T09:13:10.031329Z",
     "start_time": "2024-09-08T09:13:10.008363Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "815f629f28832a15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "model_path = \"gpt2-small\"\n",
    "save_dir = \"D:\\Data\\deception\"\n",
    "task_name = \"deception\""
   ],
   "id": "10be6ab257530688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e88f52abb15c5459"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "if task_name == \"deception\":\n",
    "    contrastive_label = (\"honest\", \"lying\")\n",
    "\n",
    "# 0. Set configuration\n",
    "model_alias = os.path.basename(model_path)\n",
    "cfg = Config(\n",
    "    model_path=model_path,\n",
    "    model_alias=model_alias,\n",
    "    task_name=task_name,\n",
    "    contrastive_label=contrastive_label,\n",
    "    save_dir=save_dir,\n",
    ")\n",
    "pprint.pp(cfg)\n",
    "artifact_dir = cfg.artifact_path()\n",
    "save_path = os.path.join(artifact_dir, \"attribution_patching\")\n"
   ],
   "id": "224d07b45de4885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 1. Load model\n",
    "model = load_model(cfg)\n",
    "model.set_use_attn_result(True)\n",
    "model.cfg.use_attn_in = True\n",
    "model.cfg.use_hook_mlp_in = True\n"
   ],
   "id": "cce663e191db4423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ioi_task_setup(\n",
    "    model,\n",
    ") -> tuple[List[str], List[tuple[str, str]]]:\n",
    "\n",
    "    prompts = [\n",
    "        \"When John and Mary went to the shops, John gave the bag to\",\n",
    "        \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "        \"When Tom and James went to the park, James gave the ball to\",\n",
    "        \"When Tom and James went to the park, Tom gave the ball to\",\n",
    "        \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
    "        \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
    "        \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
    "        \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
    "    ]\n",
    "    answers = [\n",
    "        (\" Mary\", \" John\"),\n",
    "        (\" John\", \" Mary\"),\n",
    "        (\" Tom\", \" James\"),\n",
    "        (\" James\", \" Tom\"),\n",
    "        (\" Dan\", \" Sid\"),\n",
    "        (\" Sid\", \" Dan\"),\n",
    "        (\" Martin\", \" Amy\"),\n",
    "        (\" Amy\", \" Martin\"),\n",
    "    ]\n",
    "\n",
    "    return prompts, answers\n",
    "\n",
    "\n",
    "def run_with_cache(model, clean_tokens, corrupted_tokens):\n",
    "\n",
    "    # Run the model and cache all activations\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "    return clean_logits, corrupted_logits, clean_cache, corrupted_cache\n",
    "\n",
    "\n",
    "def get_logit_diff(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_token_indices: Float[Tensor, \"batch 2\"],\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Get logit difference between the correct and incorrect answers\n",
    "    :param logits:\n",
    "    :param answer_token_indices: answer token indices for correct and incorrect answers\n",
    "    :return: a scaler for mean logit difference across samples\n",
    "    \"\"\"\n",
    "    # (1) Get last token logits only\n",
    "    logits = logits[:, -1, :]  # [batch, d_vocab]\n",
    "\n",
    "    # (2) Get the index into correct dimension\n",
    "    # squeeze because the index tensor must have the same number of dimensions as input tensor\n",
    "    correct_index = answer_token_indices[:, 0].unsqueeze(1)  # [batch, 1]\n",
    "    incorrect_index = answer_token_indices[:, 1].unsqueeze(1)  # [batch, 1]\n",
    "\n",
    "    # (3) gather the logits corresponding to the indices\n",
    "    correct_logits = logits.gather(dim=1, index=correct_index)\n",
    "    incorrect_logits = logits.gather(dim=1, index=incorrect_index)\n",
    "\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, metric, tokens):\n",
    "    filter_not_qkv_input = lambda name: \"_input\" not in name\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    def backward_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "    grad_cache = {}\n",
    "\n",
    "    model.add_hook(filter_not_qkv_input, forward_cache_hook, \"fwd\")\n",
    "    model.add_hook(filter_not_qkv_input, backward_cache_hook, \"bwd\")\n",
    "\n",
    "    value = metric(model(tokens))\n",
    "    value.backward()  # the loss for computing the backward pass\n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_attention_attr(\n",
    "    model, clean_cache, clean_grad_cache\n",
    ") -> TT[\"batch\", \"layer\", \"head_index\", \"dest\", \"src\"]:\n",
    "    attention_stack = torch.stack(\n",
    "        [clean_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    )\n",
    "    attention_grad_stack = torch.stack(\n",
    "        [clean_grad_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    )\n",
    "    attention_attr = attention_grad_stack * attention_stack\n",
    "    attention_attr = einops.rearrange(\n",
    "        attention_attr,\n",
    "        \"layer batch head_index dest src -> batch layer head_index dest src\",\n",
    "    )\n",
    "\n",
    "    return attention_attr\n",
    "\n",
    "\n",
    "class AttributionPatching:\n",
    "    def __init__(self, model, prompts, answers):\n",
    "        self.model = model\n",
    "        self.prompts = prompts\n",
    "        self.answers = answers\n",
    "        self.clean_tokens, self.corrupted_tokens, self.answer_token_indices = (\n",
    "            self.get_tokens_and_indices()\n",
    "        )\n",
    "        self.clean_baseline = None\n",
    "        self.corrupted_baseline = None\n",
    "\n",
    "        self.head_names = [\n",
    "            f\"L{l}H{h}\"\n",
    "            for l in range(self.model.cfg.n_layers)\n",
    "            for h in range(self.model.cfg.n_heads)\n",
    "        ]\n",
    "        self.head_names_signed = [\n",
    "            f\"{name}{sign}\" for name in self.head_names for sign in [\"+\", \"-\"]\n",
    "        ]\n",
    "        self.head_nams_qkv = [\n",
    "            f\"{name}{act_name}\"\n",
    "            for name in self.head_names\n",
    "            for act_name in [\"Q\", \"K\", \"V\"]\n",
    "        ]\n",
    "        print(self.head_names[:5])\n",
    "        print(self.head_names_signed[:5])\n",
    "        print(self.head_nams_qkv[:5])\n",
    "\n",
    "    def get_tokens_and_indices(\n",
    "        self,\n",
    "    ) -> tuple[\n",
    "        [Tensor, \"batch seq d_vocab\"],\n",
    "        [Tensor, \"batch seq d_vocab\"],\n",
    "        [Tensor, \"batch 2\"],\n",
    "    ]:\n",
    "        clean_tokens = self.model.to_tokens(self.prompts)\n",
    "        # Swap each adjacent pair, with a hacky list comprehension\n",
    "        corrupted_tokens = clean_tokens[\n",
    "            [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "        ]\n",
    "        print(\"Clean string 0\", self.model.to_string(clean_tokens[0]))\n",
    "        print(\"Corrupted string 0\", self.model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "        answer_token_indices = torch.tensor(\n",
    "            [\n",
    "                [self.model.to_single_token(self.answers[i][j]) for j in range(2)]\n",
    "                for i in range(len(self.answers))\n",
    "            ],\n",
    "            device=self.model.cfg.device,\n",
    "        )\n",
    "        print(\"Answer token indices\", answer_token_indices)\n",
    "\n",
    "        return clean_tokens, corrupted_tokens, answer_token_indices\n",
    "\n",
    "    def get_baseline_logit_diff(self) -> None:\n",
    "        \"\"\"\n",
    "        get baseline logit differnece for lean and corrupted inputs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        clean_logits, corrupted_logits, clean_cache, corrupted_cache = run_with_cache(\n",
    "            self.model, self.clean_tokens, self.corrupted_tokens\n",
    "        )\n",
    "        clean_logit_diff = get_logit_diff(\n",
    "            clean_logits, self.answer_token_indices\n",
    "        ).item()\n",
    "        print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "        corrupted_logit_diff = get_logit_diff(\n",
    "            corrupted_logits, self.answer_token_indices\n",
    "        ).item()\n",
    "        print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "\n",
    "        self.clean_baseline = clean_logit_diff\n",
    "        self.corrupted_baseline = corrupted_logit_diff\n",
    "\n",
    "        print(f\"Clean Baseline is 1: {self.ioi_metric(clean_logits).item():.4f}\")\n",
    "        print(\n",
    "            f\"Corrupted Baseline is 0: {self.ioi_metric(corrupted_logits).item():.4f}\"\n",
    "        )\n",
    "\n",
    "    def ioi_metric(\n",
    "        self,\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Scale the logit difference between 0 and 1 (clean baseline will have logit diff =1, and corrupted baseline will have logit diff =0)\n",
    "        :param logits:\n",
    "        :param answer_token_indices:\n",
    "        :param clean_logit_diff:\n",
    "        :param corrupted_logit_diff:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            get_logit_diff(logits, self.answer_token_indices) - self.corrupted_baseline\n",
    "        ) / (self.clean_baseline - self.corrupted_baseline)\n",
    "\n",
    "    def get_attention_attribution(self):\n",
    "        clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(\n",
    "            self.model, self.ioi_metric, self.clean_tokens\n",
    "        )\n",
    "        corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(\n",
    "            self.model, self.ioi_metric, self.corrupted_tokens\n",
    "        )\n",
    "        attention_attr = create_attention_attr(\n",
    "            self.model, clean_cache, clean_grad_cache\n",
    "        )\n",
    "        return attention_attr\n",
    "\n",
    "    def plot_attention_attr(\n",
    "        self, attention_attr, tokens, top_k=20, index=0, title=\"\", save_path=None\n",
    "    ):\n",
    "        if len(tokens.shape) == 2:\n",
    "            tokens = tokens[index]\n",
    "        if len(attention_attr.shape) == 5:\n",
    "            attention_attr = attention_attr[index]\n",
    "        attention_attr_pos = attention_attr.clamp(min=-1e-5)\n",
    "        attention_attr_neg = -attention_attr.clamp(max=1e-5)\n",
    "        attention_attr_signed = torch.stack(\n",
    "            [attention_attr_pos, attention_attr_neg], dim=0\n",
    "        )\n",
    "        attention_attr_signed = einops.rearrange(\n",
    "            attention_attr_signed,\n",
    "            \"sign layer head_index dest src -> (layer head_index sign) dest src\",\n",
    "        )\n",
    "        attention_attr_signed = attention_attr_signed / attention_attr_signed.max()\n",
    "        attention_attr_indices = (\n",
    "            attention_attr_signed.max(-1).values.max(-1).values.argsort(descending=True)\n",
    "        )\n",
    "        # print(attention_attr_indices.shape)\n",
    "        # print(attention_attr_indices)\n",
    "        attention_attr_signed = attention_attr_signed[attention_attr_indices, :, :]\n",
    "        head_labels = [self.head_names_signed[i.item()] for i in attention_attr_indices]\n",
    "\n",
    "        if title:\n",
    "            display(Markdown(\"### \" + title))\n",
    "\n",
    "        vis = pysvelte.AttentionMulti(\n",
    "            tokens=self.model.to_str_tokens(tokens),\n",
    "            attention=attention_attr_signed.permute(1, 2, 0)[:, :, :top_k],\n",
    "            head_labels=head_labels[:top_k],\n",
    "        )\n",
    "\n",
    "        # save the visualization as .html file\n",
    "        vis_path = save_path + os.sep + \"attention_attribution\" + os.sep + f\"top20.html\"\n",
    "        if not os.path.exists(vis_path):\n",
    "            os.makedirs(vis_path)\n",
    "        with open(vis_path, \"w\") as f:\n",
    "            f.write(vis)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_path: str,\n",
    "    save_dir: str,\n",
    "    task_name: str = \"deception\",\n",
    "    contrastive_label: Tuple[str] = (\"honest\", \"lying\"),\n",
    "):\n",
    "\n",
    "\n",
    "    # 1. Load model\n",
    "    model = load_model(cfg)\n",
    "    model.set_use_attn_result(True)\n",
    "    model.cfg.use_attn_in = True\n",
    "    model.cfg.use_hook_mlp_in = True\n",
    "\n",
    "    # 2. ioi_task_setup\n",
    "    print(\"loading data\")\n",
    "    prompts, answers = ioi_task_setup(model)\n",
    "\n",
    "    # initiate\n",
    "    attribution_patching = AttributionPatching(model, prompts, answers)\n",
    "    # get baseline logit diff\n",
    "    attribution_patching.get_baseline_logit_diff()\n",
    "    # get attention attribution\n",
    "    attention_attr = attribution_patching.get_attention_attribution()\n",
    "    # visualize attention pattern\n",
    "    attribution_patching.plot_attention_attr(\n",
    "        attention_attr,\n",
    "        attribution_patching.clean_tokens,\n",
    "        top_k=20,\n",
    "        index=0,\n",
    "        title=\"\",\n",
    "        save_path=save_path,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb447bd50f0c3af7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
